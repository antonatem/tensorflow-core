{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, preprocessing, models, layers, optimizers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUTTLE CAPTAIN: Command station, this is ST 321. Code Clearance Blue. We're starting our approach. Deactivate the security shield.DEATH STAR CONTROLLER: The security deflector shield will be deactivated when we have confirmation of your code transmi\n"
     ]
    }
   ],
   "source": [
    "# Read star wars movie script\n",
    "text = open('data/star_wars_episode_vi.txt', 'rb').read().decode(encoding='utf8')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 27 40 39 39 31 24  1 22 20 35 39 20 28 33 17  1 22 60 58 58 46 59 49\n",
      "  1 64 65 46 65 54 60 59  6  1 65 53 54 64  1 54 64  1 38 39  1 13 12 11\n",
      "  8  1 22 60 49 50  1 22 57 50 46 63 46 59 48 50  1 21 57 66 50  8  1 42\n",
      " 50  5 63 50  1 64 65 46 63 65 54 59 52  1 60 66 63  1 46 61 61 63 60 46\n",
      " 48 53  8  1 23 50 46 48 65 54 67 46 65 50  1 65 53 50  1 64 50 48 66 63\n",
      " 54 65 70  1 64 53 54 50 57 49  8 23 24 20 39 27  1 38 39 20 37  1 22 34\n",
      " 33 39 37 34 31 31 24 37 17  1 39 53 50  1 64 50 48 66 63 54 65 70  1 49\n",
      " 50 51 57 50 48 65 60 63  1 64 53 54 50 57 49  1 68 54 57 57  1 47 50  1\n",
      " 49 50 46 48 65 54 67 46 65 50 49  1 68 53 50 59  1 68 50  1 53 46 67 50\n",
      "  1 48 60 59 51 54 63 58 46 65 54 60 59  1 60 51  1 70 60 66 63  1 48 60\n",
      " 49 50  1 65 63 46 59 64 58 54]\n"
     ]
    }
   ],
   "source": [
    "# Check unique characters\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# Create mapping for unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"SHUTTLE CAPTAIN: Command station, this is ST 321. Code Clearance Blue. We're starting our approach. D\"\n",
      "'eactivate the security shield.DEATH STAR CONTROLLER: The security deflector shield will be deactivate'\n",
      "'d when we have confirmation of your code transmission. Stand by... You are clear to proceed.\\nSHUTTLE '\n",
      "\"CAPTAIN: We're starting our approach.\\nOFFICER: Inform the commander that Lord Vader's shuttle has arr\"\n",
      "\"ived.\\nOPERATOR: Yes, sir.\\nJERJERROD: Lord Vader, this is an unexpected pleasure.  We're honored by yo\"\n"
     ]
    }
   ],
   "source": [
    "# Set maximum sequence length\n",
    "seq_len = 100\n",
    "examples_per_epoch = len(text)\n",
    "\n",
    "# Create training examples and targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Convert sequence of characters to sequences of set seq_length\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input example: \"SHUTTLE CAPTAIN: Command station, this is ST 321. Code Clearance Blue. We're starting our approach. \"\n",
      "Target example: \"HUTTLE CAPTAIN: Command station, this is ST 321. Code Clearance Blue. We're starting our approach. D\"\n"
     ]
    }
   ],
   "source": [
    "# For each sequence, duplicate and shift it to form the input and target text\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input example: {}\".format(repr(''.join(idx2char[input_example]))))\n",
    "    print(\"Target example: {}\".format(repr(''.join(idx2char[target_example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 256)           18432     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 72)            73800     \n",
      "=================================================================\n",
      "Total params: 4,030,536\n",
      "Trainable params: 4,030,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dimension = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dimension, rnn_units, batch_size):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dimension, \n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, \n",
    "                            stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, embedding_dimension, rnn_units, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Save checkpoints during training\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 9s 1s/step - loss: 4.8260\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 9s 1s/step - loss: 3.9724\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 3.7916\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 3.4834\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 3.2389\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 3.1153\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 2.9580\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 11s 2s/step - loss: 2.7761\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 2.6157\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.5086\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.4392\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.3781\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.3288\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.2919\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.2513\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.2240\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.1929\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.1685\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 2.1355\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 2.1123\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 12s 2s/step - loss: 2.0913\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.0641\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.0472\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.0229\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1.9914\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1.9781\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.9459\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.9229\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.8978\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.8822\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.8489\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.8261\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.7971\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.7747\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.7543\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.7267\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1.7035\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.6786\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.6522\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.6262\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.6007\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.5715\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.5551\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.5199\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.5012\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.4714\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.4412\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1.4145\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.3938\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.3653\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (1, None, 256)            18432     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, None, 72)             73800     \n",
      "=================================================================\n",
      "Total params: 4,030,536\n",
      "Trainable params: 4,030,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Rebuild model with batch size = 1\n",
    "model = build_model(vocab_size, embedding_dimension, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input your text: OFFICER\n",
      "OFFICER: You want the mound the dorks are you will ne to know the fire come on the conitht my father.\n",
      "VADER: It is will come of the shield sere.\n",
      "HAN: What's gonna net it it they don't the oll the Emperor will nee go stack as the to seep that thing.\n",
      "THREEPIO: I am the sait in the pleet.\n",
      "VADER: Your mestroy here awer angere and to the matien fire of the ranged har.\n",
      "THREEPIO: Oh, yes, my Master Luke!  We're now the Emperor has a reade at the farce and this.\n",
      "HAN: We ale you all the stickn's be a proined be not to all refurate ald I have the fliends wan you and the dest.\n",
      "HAN: We're gon a get a Jedi's come on the macker stack of the parress and thing. That shield he wirn you will you are your father has for a gith they den't you a Jedi's all time. I can te the going to the reach power are a gring a preath the pare of the dighter sourte.\n",
      "THREEPIO: Oh, my you're of the eas.\n",
      "BEN: You're got in the surce is staiding a fill bittle with your dething with a Jedi will sime. I will not the dast conflete tho\n"
     ]
    }
   ],
   "source": [
    "# Generate text from model predictions\n",
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Vectorize start string\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "input_string = input(\"Input your text: \")\n",
    "print(generate_text(model, start_string=input_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
